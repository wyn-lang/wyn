// Lexer module for Wyn - Exportable tokenization
// Extracted from lexer.wyn for modular compiler
// Uses int constants instead of enums to avoid codegen issues

// Token structure
struct Token {
    tok_type: int,
    value: string
}

// Token type constants
const TokenType_EndOfFile = 0
const TokenType_Ident = 1
const TokenType_Int = 2
const TokenType_String = 3
const TokenType_Fn = 4
const TokenType_Var = 5
const TokenType_If = 6
const TokenType_Else = 7
const TokenType_While = 8
const TokenType_Return = 9
const TokenType_Plus = 10
const TokenType_Minus = 11
const TokenType_Star = 12
const TokenType_Slash = 13
const TokenType_Equal = 14
const TokenType_EqualEqual = 15
const TokenType_Less = 16
const TokenType_Greater = 17
const TokenType_Arrow = 18
const TokenType_Dot = 19
const TokenType_Colon = 20
const TokenType_LParen = 21
const TokenType_RParen = 22
const TokenType_LBrace = 23
const TokenType_RBrace = 24
const TokenType_Comma = 25
const TokenType_Semicolon = 26
const TokenType_Unknown = 27

fn is_digit(ch: string) -> int {
    if ch == "0" { return 1 }
    if ch == "1" { return 1 }
    if ch == "2" { return 1 }
    if ch == "3" { return 1 }
    if ch == "4" { return 1 }
    if ch == "5" { return 1 }
    if ch == "6" { return 1 }
    if ch == "7" { return 1 }
    if ch == "8" { return 1 }
    if ch == "9" { return 1 }
    return 0
}

fn is_alpha(ch: string) -> int {
    if ch == "a" { return 1 }
    if ch == "b" { return 1 }
    if ch == "c" { return 1 }
    if ch == "d" { return 1 }
    if ch == "e" { return 1 }
    if ch == "f" { return 1 }
    if ch == "g" { return 1 }
    if ch == "h" { return 1 }
    if ch == "i" { return 1 }
    if ch == "j" { return 1 }
    if ch == "k" { return 1 }
    if ch == "l" { return 1 }
    if ch == "m" { return 1 }
    if ch == "n" { return 1 }
    if ch == "o" { return 1 }
    if ch == "p" { return 1 }
    if ch == "q" { return 1 }
    if ch == "r" { return 1 }
    if ch == "s" { return 1 }
    if ch == "t" { return 1 }
    if ch == "u" { return 1 }
    if ch == "v" { return 1 }
    if ch == "w" { return 1 }
    if ch == "x" { return 1 }
    if ch == "y" { return 1 }
    if ch == "z" { return 1 }
    return 0
}

fn is_whitespace(ch: string) -> int {
    if ch == " " { return 1 }
    if ch == "\n" { return 1 }
    if ch == "\t" { return 1 }
    return 0
}

fn check_keyword(input: string, start: int, len: int) -> int {
    if len == 2 {
        if input[start] == "f" {
            if input[start + 1] == "n" {
                return TokenType_Fn
            }
        }
        if input[start] == "i" {
            if input[start + 1] == "f" {
                return TokenType_If
            }
        }
    }
    
    if len == 3 {
        if input[start] == "v" {
            if input[start + 1] == "a" {
                if input[start + 2] == "r" {
                    return TokenType_Var
                }
            }
        }
    }
    
    if len == 4 {
        if input[start] == "e" {
            if input[start + 1] == "l" {
                if input[start + 2] == "s" {
                    if input[start + 3] == "e" {
                        return TokenType_Else
                    }
                }
            }
        }
    }
    
    if len == 5 {
        if input[start] == "w" {
            if input[start + 1] == "h" {
                if input[start + 2] == "i" {
                    if input[start + 3] == "l" {
                        if input[start + 4] == "e" {
                            return TokenType_While
                        }
                    }
                }
            }
        }
    }
    
    if len == 6 {
        if input[start] == "r" {
            if input[start + 1] == "e" {
                if input[start + 2] == "t" {
                    if input[start + 3] == "u" {
                        if input[start + 4] == "r" {
                            if input[start + 5] == "n" {
                                return TokenType_Return
                            }
                        }
                    }
                }
            }
        }
    }
    
    return TokenType_Ident
}

export fn tokenize(input: string) -> [Token] {
    var tokens: [Token] = []
    var pos = 0
    
    while pos < input.len() {
        var ch = input[pos]
        
        if is_whitespace(ch) {
            pos = pos + 1
        } else if is_digit(ch) {
            var start = pos
            while pos < input.len() && is_digit(input[pos]) {
                pos = pos + 1
            }
            tokens.push(Token { tok_type: TokenType_Int, value: input.substring(start, pos) })
        } else if is_alpha(ch) {
            var start = pos
            while pos < input.len() && is_alpha(input[pos]) {
                pos = pos + 1
            }
            var len = pos - start
            var tok_type = check_keyword(input, start, len)
            tokens.push(Token { tok_type: tok_type, value: input.substring(start, pos) })
        } else if ch == "+" {
            tokens.push(Token { tok_type: TokenType_Plus, value: "+" })
            pos = pos + 1
        } else if ch == "-" {
            if pos + 1 < input.len() && input[pos + 1] == ">" {
                tokens.push(Token { tok_type: TokenType_Arrow, value: "->" })
                pos = pos + 2
            } else {
                tokens.push(Token { tok_type: TokenType_Minus, value: "-" })
                pos = pos + 1
            }
        } else if ch == "*" {
            tokens.push(Token { tok_type: TokenType_Star, value: "*" })
            pos = pos + 1
        } else if ch == "/" {
            tokens.push(Token { tok_type: TokenType_Slash, value: "/" })
            pos = pos + 1
        } else if ch == "=" {
            if pos + 1 < input.len() && input[pos + 1] == "=" {
                tokens.push(Token { tok_type: TokenType_EqualEqual, value: "==" })
                pos = pos + 2
            } else {
                tokens.push(Token { tok_type: TokenType_Equal, value: "=" })
                pos = pos + 1
            }
        } else if ch == "<" {
            tokens.push(Token { tok_type: TokenType_Less, value: "<" })
            pos = pos + 1
        } else if ch == ">" {
            tokens.push(Token { tok_type: TokenType_Greater, value: ">" })
            pos = pos + 1
        } else if ch == "." {
            tokens.push(Token { tok_type: TokenType_Dot, value: "." })
            pos = pos + 1
        } else if ch == ":" {
            tokens.push(Token { tok_type: TokenType_Colon, value: ":" })
            pos = pos + 1
        } else if ch == "(" {
            tokens.push(Token { tok_type: TokenType_LParen, value: "(" })
            pos = pos + 1
        } else if ch == ")" {
            tokens.push(Token { tok_type: TokenType_RParen, value: ")" })
            pos = pos + 1
        } else if ch == "{" {
            tokens.push(Token { tok_type: TokenType_LBrace, value: "{" })
            pos = pos + 1
        } else if ch == "}" {
            tokens.push(Token { tok_type: TokenType_RBrace, value: "}" })
            pos = pos + 1
        } else if ch == "," {
            tokens.push(Token { tok_type: TokenType_Comma, value: "," })
            pos = pos + 1
        } else if ch == ";" {
            tokens.push(Token { tok_type: TokenType_Semicolon, value: ";" })
            pos = pos + 1
        } else {
            pos = pos + 1
        }
    }
    
    tokens.push(Token { tok_type: 0, value: "" })
    return tokens
}

fn main() -> int {
    var tokens = tokenize("var x = 42")
    print("Lexer module test complete")
    return 0
}
